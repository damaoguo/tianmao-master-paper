\chapter{基于增量学习的新移动应用识别方法}\label{chap:transfer}
\section{引言}
目前的加密流量识别方法具有一个主要缺点，那就是新应用程序继续遭受\textit{灾难性遗忘}\citep{french1999catastrophic,robins1995catastrophic,kirkpatrick2017overcoming}的困扰，当逐步增加新的应用程序类别进行培训时，整体性能会急剧下降。当前已有的研究工作均是需要全部的流量数据来训练，针对新出现的应用，需要将原来的流量数据和新来的应用的流量数据合并一起重新进行训练，当应用不断出现，训练的样本数量会急剧增加，这将给流量数据的存储以及模型的训练带来巨大的压力。为了解决这个问题，提出\emph{IncreAIBMF}框架，以使用新的应用程序数据以及仅对应于旧应用程序样本的一小样本集逐步学习深度神经网络。

为了验证我们提出的IncreAIBMF模型的优越性，在本章本章将IncreAIBMF模型分别和针对AIBMF模型微调、全量训练进行比较。



\section{方法描述}

\subsection{增量学习框架}
如图：\ref{fig:IncreAIBMF-architecture}所示，IncreAIBMF由三个模块构成：训练集构建，训练，代表性存储空间。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.95\textwidth]{IncreAIBMF.pdf}
	\bicaption{IncreAIBMF架构}{IncreAIBMF architecture}
	\label{fig:IncreAIBMF-architecture}
\end{figure}

分别介绍如下：
\subsubsection{构建训练集}
此阶段准备训练数据以用于下一个训练阶段。训练集由旧类的代表性样本的一部分（代表存储器中存储的旧类的示例）和新的app类的样本组成。由于我们的方法使用了两个损失函数，即分类和精馏，因此每个样本都需要两个标签，与这两个损失相关联。为了进行分类，我们使用one-hot向量来指示应用程序出现在所有应用程序类中。对于精馏，使用每个识别模块使用旧的应用程序类生成的精馏标签。因此，每个样品的蒸馏标签与具有旧应用程序类的识别模块一样多。此时，两个标识模块将在旧应用程序上运行，一个标识模块将处理新应用程序。对样品进行评估时，将使用旧应用程序的两个识别模块生成的精馏标签用于精馏，将使用三个识别模块生成的精馏标签用于分类。


\subsubsection{训练过程}
训练阶段将带有相应标签的增强训练集作为交叉蒸馏损失计算的输入，并更新整个网络体系结构的所有参数。 交叉蒸馏损失函数L定义如公式：（\ref{eqn:cdl}）所示。
\begin{equation}
	\label{eqn:cdl}
	L(\omega)=L_{C}(\omega)+\lambda\sum_{f=1}^{F} L_{D_{f}}(\omega)
\end{equation}

这里 $L_{C}(\omega)$是应用于新旧类别样本的交叉熵损失。
%
$ L_{D_{f}} $是分类层$ f $的蒸馏损失，$ F $是旧应用程序的分类层总数。
%
$ \lambda $是指交叉蒸馏损失中蒸馏损失的比率。 在训练过程中，我们将其设置为0.3。
%
交叉熵损失$ L_{C}（\omega）$由下式给出：
%
\begin{equation}
	L_{C}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} p_{i j} \log q_{i j}
\end{equation}
%
其中$ qi $是通过对样本$ i $的分类层的logit应用softmax函数获得的分数，$ pi $是样本$ i $的基本事实，$ N $和$ C $表示 样本数和类别。
%
精馏损失 $L_{D}(\omega)$ 定义为:
%
\begin{equation}
	L_{D}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} { pdist }_{i j} \log {qd i s t}_{i j}
\end{equation}
%
其中$ pdisti $和$ qdisti $分别是$ pi $和$ qi $的修改版本。

\subsubsection{代表性空间更新}
此阶段的目标是更新内存中的样本，并确保训练集的大小不会急剧增加。也就是说，在更新的内存中，本章将内存大小固定为$ K $个流量（本文中$ K = 100,000 $），并且与每个应用程序类别相对应的样本数为$ \frac{100,000}{C} $，此处$ C $表示应用程序的类别。

然后，采用两个操作：

\begin{itemize}
    \item 选择要存储的新样本：执行herding采样\citep{welling2009herding}，该策略根据与该类别的平均样本的距离生成一类样本的排序列表；给定样本的排序列表，选择列表中的前$ n $个样本。这些样本根据平均值最能代表该类别。
    \item 删除剩余的样本：由于样本存储在排序列表中，因此此操作很简单。存储单元仅需要从每个类别的样本集的末尾删除样本。请注意，执行此操作后，删除的样本将不再使用。 
\end{itemize}


\section{实验评估}

\subsection{微调效果}
面对越来越多的应用程序，常用的方法之一是微调。微调方法是指基于训练后的模型添加少量任务特定的参数。例如，对于分类问题，将softmax网络层添加到模型中，然后对新类进行重新训练以进行微调。对于新收集的30个应用程序的流量，本文首先使用微调的想法，冻结在20个旧应用程序上训练的AIBMF网络模型的参数以进行特征提取，并且仅修改完全连接的层数。在每个训练步骤添加$L \in \emph{[5,10,15,20,25,30]}$个应用。如图\ref {fig:AIBMF-Fine-tuning}所示，新模型无法完成旧类别和新类别中的识别任务，本文将这种糟糕的表现归因于灾难性的遗忘。迁移学习是一种处理新数据的有效方法，它普遍使用的场景为：原始模型是通过大量的数据训练得到的，处理的新数据是更为小范围的数据，即原始模型本身训练了更多的数据，具有更强的特征建模能力。在我们的研究过程中，应用不断出现，原始模型的数量要远远少于新增数据的数量，另外实验结果表明，微调的方法并不能够很好处理本研究课题面对的新增应用数量巨大带来的问题。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{AIBMF-Fine-tuning.pdf}
	\bicaption{AIBMF微调识别效果}{AIBMF Fine tuning}
	\label{fig:AIBMF-Fine-tuning}
\end{figure}

\subsection{增量学习效果}
本章使用第四章所训练的AIBMF模型筛选出原始的代表性空间，并将该空间的大小设置为10条样本，筛选的方法为通过该模型计算出全连接层的向量，计算出所有样本计算的全连接层向量的中心点，保留前$100000/K$的距离中心点最近的样本，在这里100000表示的是代表性空间的大小，$K$代表的是类别数目，如一开始$K=20$，则在构建代表性空间的时候，每一个应用被保留的样本的数量将会$\leq 5000$，即每一个应用距离中心点的前5000个样本会被保留，不够5000的将会被全部保留。本章使用的评估指标和实验环境与第四章相同。


为了验证基于增量学习的模型IncreAIBMF模型在所有50个应用20多万条流的样本上识别效果，本章首先使用第四章所述的AIBMF模型对50个应用一次性进行训练，其中18条流用于训练，2万多条流用于验证。

图：\ref{fig:Time-cost}对比了AIBMF模型和Increment模型在随着样本数量增加的时候训练所需要的时间变化情况。对于AIBMF模型，新应用增加的时候，模型需要重新对整个数据集进行训练，训练的样本的数量会持续增加，因此图中AIBMF模型的训练时间消耗随着应用的出现持续增加。而IncreAIBMF模型限制了训练集的规模，当新的应用出现的时候，训练集的规模并不会爆炸增加，因此图中IncreAIBMF模型的训练时间消耗随着新应用的出现变化不大。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{Time-cost-incre.eps}
	\bicaption{AIBMF和IncreAIBM时间消耗对比 }{Time cost comparation between AIBMF and IncreAIBMF}
	\label{fig:Time-cost}
\end{figure}

图：\ref{fig:AIBMF-IncreAIBMF}是AIBMF和IncreAIBMF模型在识别效果上的对比，共包含了精确度（Mpre）、Fi值（MF1）和召回率（Mrec）三个标准。其中图：\ref{fig:AIBMF-performance}是AIBMF在应用类别增加的时候的识别效果变化，可见AIBMF在应用类别增加的时候具备鲁棒性。图：\ref{fig:IncreAIBMF-performance}是IncreAIBMF模型在应用类别增加的时候的识别效果变化，和AIBMF相比，识别性能的差距很微弱，即IncreAIBMF模型保持了AIBMF的识别性能。
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Full-comparation.eps}
      \caption{}
      \label{fig:AIBMF-performance}
    \end{subfigure}%
    ~% add desired spacing
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Incre-comparation.eps}
      \caption{}
      \label{fig:IncreAIBMF-performance}
    \end{subfigure}
    ~% add desired spacing
    \bicaption{AIBMF和IncreAIBMF识别效果对比。(a) AIBMF识别效果，(b) IncreAIBMF识别效果。}{Comparation of the Performance between AIBMF and IncreAIBMF.(a) AIBMF Performance, (b) IncreAIBMF Performance.}
    \label{fig:AIBMF-IncreAIBMF}
\end{figure}


图：\ref{fig:IncreAIBM-confuse-matrix}是IncreAIBMF模型在50个应用，2万余条样本上的识别效果。在大多数情况下，识别性能几乎是完美的，错误是依然是由于不同应用之间的网络服务共享所致，尤其是同一开发人员开发的应用（例如”闲鱼“和”阿里巴巴“均由AliBaBa.com开发，”闲鱼“被IncreAIBMF错误地识别为”阿里健康“）。另外”天涯“的用户量很少，检查了数据集的组成，发现”天涯“的样本很少（因为该应用不再活跃且大部分通信没有使用HTTPS协议），对其识别效果差是数据不均衡造成的。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.90\textwidth]{Incre-confuse-matrix.pdf}
	\bicaption{IncreAIBM 识别混淆矩阵}{IncreAIBM confuse matrix}
	\label{fig:IncreAIBM-confuse-matrix}
\end{figure}



\section{本章小节}
这一部分提出了IncreAIBMF模型，有效处理新增应用的流量数据的识别，使得我们的识别模型能够有效克服灾难遗忘问题，在新增应用的识别任务上，能够有效节约时间和空间，更加高效完成识别任务。