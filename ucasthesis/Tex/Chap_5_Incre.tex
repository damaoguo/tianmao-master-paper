\chapter{基于增量学习的新增移动应用识别方法}\label{chap:transfer}
\section{引言}
目前的加密流量识别方法具有一个主要缺点是新应用继续遭受\textit{灾难性遗忘}\citep{french1999catastrophic,robins1995catastrophic,kirkpatrick2017overcoming}的困扰，当逐步增加新的应用类别进行培训时，整体性能会急剧下降。当前已有的研究工作均是需要全部的流量数据来训练，针对新出现的应用，需要将原来的流量数据和新来的应用的流量数据合并一起重新进行训练，当应用不断出现，训练的样本数量会急剧增加，这将给流量数据的存储以及模型的训练带来巨大的压力。为了解决这个问题，本文提出一种基于增量学习的新增移动应用识别方法---\emph{IncreAIBMF}\footnote{\underline{Incre}mental Learning for \underline{A}pplication \underline{I}dentification Over HTTPS Traffic \underline{B}ased on \underline{M}ulti-view \underline{F}eatures}，使用新增的应用流量数据以及旧应用代表性流量样本，结合旧类别识别模型逐步学习深度神经网络。为了验证本章提出的IncreAIBMF模型的有效性，在本章将IncreAIBMF模型分别和针对AIBMF模型微调、全量训练进行比较。


\section{方法阐述}

如图\ref{fig:IncreAIBMF-architecture}所示，IncreAIBMF由三个步骤构成：训练集构建，训练，代表性流量样本更新，分别介绍如下：
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.95\textwidth]{IncreAIBMF.pdf}
	\bicaption{IncreAIBMF架构}{IncreAIBMF architecture}
	\label{fig:IncreAIBMF-architecture}
\end{figure}


\subsection{构建训练集}
训练集的构建是为下一个训练阶段准备训练集。训练集由旧应用类别的一部分即代表性流量样本和新增的应用的流量样本组成。由于本章提出的IncreAIBMF方法使用了两个损失函数，即分类损失和精馏损失，因此每个样本都需要两个标签，与这两个损失相关联。为了进行分类，使用one-hot向量来指示应用出现在所有应用类中；对于精馏，使用每个识别模块使用旧的应用类生成的精馏标签，精馏标签是通过将训练集使用旧模型计算出的概率值，每一个标签是一个向量。


\subsection{训练过程}
训练阶段将带有相应标签的训练集作为交叉精馏损失计算的输入，并更新整个网络体系结构的所有参数。交叉精馏损失函数L定义如公式\ref{eqn:cdl}所示。
\begin{equation}
	\label{eqn:cdl}
	L(\omega)=L_{C}(\omega)+\lambda\sum_{f=1}^{F} L_{D_{f}}(\omega)
\end{equation}

这里 $L_{C}(\omega)$是应用于新旧类别样本的交叉熵损失。
%
$ L_{D_{f}} $是分类层$ f $的蒸馏损失，$ F $是旧应用的分类层总数。
%
$ \lambda $是指交叉蒸馏损失中蒸馏损失的比率，在训练过程中，本章将其设置为0.3。
%
交叉熵损失$ L_{C}(\omega)$由下式给出：
%
\begin{equation}
	L_{C}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} p_{i j} \log q_{i j}
\end{equation}
%
其中$ qi $是通过对样本$ i $的分类层的logit应用softmax函数计算得到的值，$ pi $是样本$ i $的概率，$ N $和$ C $表示 样本数和类别。
%
精馏损失 $L_{D}(\omega)$ 定义为:
%
\begin{equation}
	L_{D}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} { pdist }_{i j} \log {qd i s t}_{i j}
\end{equation}
%
其中$ pdisti $和$ qdisti $分别是$ pi $和$ qi $的变形。

\subsection{代表性流样本更新}
此阶段的目标是更新内存中的样本，并确保训练集的大小不会急剧增加。也就是说，在更新的内存中，本章将内存大小固定为$ K $个流量样本（本文中$ K = 100,000 $），并且与每个应用类别相对应的样本数为$ \frac{100,000}{C} $，此处$ C $表示应用的类别。然后，采用两个操作：

\begin{itemize}
    \item 选择要存储的新样本：执行herding采样\citep{welling2009herding}，该策略根据与该类别的平均样本的距离生成一类样本的排序列表；给定样本的排序列表，选择列表中的前$n$个样本。这些样本根据平均值最能代表该类别。
    \item 删除剩余的样本：由于样本存储在排序列表中，因此此操作很简单。存储单元仅需要从每个类别的样本集的末尾删除样本。请注意，执行此操作后，删除的样本将不再使用。 
\end{itemize}


\section{实验评估}

\subsection{微调效果}
面对越来越多的应用，常用的方法之一是微调。微调方法是指基于训练后的模型添加少量任务特定的参数。例如，对于分类问题，将softmax网络层添加到模型中，然后对新类进行重新训练以进行微调。对于新收集的30个应用的流量，本文首先使用微调的想法，冻结在20个旧应用上训练的AIBMF网络模型的参数以进行特征提取，并且仅修改完全连接的层数。在每个训练步骤添加$L \in \emph{[5,10,15,20,25,30]}$个应用。如图\ref {fig:AIBMF-Fine-tuning}所示，新模型无法完成旧类别和新类别中的识别任务，本文将这种糟糕的表现归因于灾难性的遗忘。在真实的应用环境中，应用不断出现，原始模型的数量要远远少于新增数据的数量，另外实验结果表明，微调的方法并不能够很好处理本研究课题面对的新增应用数量巨大带来的问题。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{AIBMF-Fine-tuning.pdf}
	\bicaption{AIBMF微调识别效果}{AIBMF Fine tuning}
	\label{fig:AIBMF-Fine-tuning}
\end{figure}

\subsection{增量学习效果}
本章使用第四章所训练的AIBMF模型筛选出原始的代表性空间，并将该空间的大小设置为10条样本，筛选的方法为通过该模型计算出全连接层的向量，计算出所有样本计算的全连接层向量的中心点，保留前$100000/K$的距离中心点最近的样本，在这里100000表示的是代表性空间的大小，$K$代表的是类别数目，如一开始$K=20$，则在构建代表性空间的时候，每一个应用被保留的样本的数量将会$\leq5000$，即每一个应用距离中心点的前5000个样本会被保留，不够5000的将会被全部保留。本章使用的评估指标和实验环境与第四章相同。


为了验证基于增量学习的模型IncreAIBMF模型在所有50个应用20多万条流的样本上识别效果，本章首先使用第四章所述的AIBMF模型对50个应用一次性进行训练，其中18条流用于训练，2万多条流用于验证。

图\ref{fig:Time-cost}对比了AIBMF模型和Increment模型在随着样本数量增加的时候训练所需要的时间变化情况。对于AIBMF模型，新应用增加的时候，模型需要重新对整个数据集进行训练，训练的样本的数量会持续增加，因此图中AIBMF模型的训练时间消耗随着应用的出现持续增加。而IncreAIBMF模型限制了训练集的规模，当新的应用出现的时候，训练集的规模并不会爆炸增加，因此图中IncreAIBMF模型的训练时间消耗随着新应用的出现变化不大。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{Time-cost-incre.eps}
	\bicaption{AIBMF和IncreAIBM时间消耗对比 }{Time cost comparation between AIBMF and IncreAIBMF}
	\label{fig:Time-cost}
\end{figure}

图\ref{fig:AIBMF-IncreAIBMF}是AIBMF和IncreAIBMF模型在识别效果上的对比，共包含了精度（Mpre）、宏$F_1$值（MF1）和宏召回率（Mrec）三个标准。
\begin{itemize}
    \item 图\ref{fig:AIBMF-performance}是AIBMF在第四章的20个应用的数据集上，每次增加5个应用，将新旧应用数据一起训练的识别效果变化，AIBMF模型并没有因为识别类别的增加而出现识别能力骤降的问题，可见AIBMF在应用类别增加的时候具备鲁棒性。本章提出的IncreAIBMF模型是在AIBMF模型上就训练过程进行了改进，在这里，一次性训练全部的数据确立了AIBMF相关模型识别能力的上界（up bound）。
    \item 图\ref{fig:IncreAIBMF-performance}是IncreAIBMF模型在应用类别增加的时候的识别效果变化，和AIBMF相比，识别性能的差距很微弱，即使当应用的数量增加到50个时，IncreAIBMF在宏精度、宏F1值和宏召回率分别取得了87.3\%，87.8\%，88.9\%，即IncreAIBMF模型保持了AIBMF的识别性能。
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Full-comparation.eps}
      \caption{}
      \label{fig:AIBMF-performance}
    \end{subfigure}%
    ~% add desired spacing
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Incre-comparation.eps}
      \caption{}
      \label{fig:IncreAIBMF-performance}
    \end{subfigure}
    ~% add desired spacing
    \bicaption{AIBMF和IncreAIBMF识别效果对比。(a) AIBMF识别效果，(b) IncreAIBMF识别效果。}{Comparation of the Performance between AIBMF and IncreAIBMF.(a) AIBMF Performance, (b) IncreAIBMF Performance.}
    \label{fig:AIBMF-IncreAIBMF}
\end{figure}


图\ref{fig:IncreAIBM-confuse-matrix}是IncreAIBMF模型在50个应用，2万余条样本上的识别效果。在大多数情况下，识别性能几乎是完美的，错误是依然是由于不同应用之间的网络服务共享所致，尤其是同一开发人员开发的应用（例如”闲鱼“和”阿里巴巴“均由AliBaBa.com开发，”闲鱼“被IncreAIBMF错误地识别为”阿里健康“）。另外”天涯“的用户量很少，检查了数据集的组成，发现”天涯“的样本很少（因为该应用不再活跃且大部分通信没有使用HTTPS协议），对其识别效果差是数据不均衡造成的。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.65\textwidth]{Incre-confuse-matrix.pdf}
	\bicaption{IncreAIBM 识别混淆矩阵}{IncreAIBM confuse matrix}
	\label{fig:IncreAIBM-confuse-matrix}
\end{figure}


\section{本章小结}
这一部分提出了IncreAIBMF模型，通过使用增量学习改进第四章提出的AIBMF模型，有效处理新增应用的流量数据的识别，使识别模型能够有效克服灾难遗忘问题，在新增应用的识别任务上，能够有效节约时间和空间，更加高效完成识别任务。