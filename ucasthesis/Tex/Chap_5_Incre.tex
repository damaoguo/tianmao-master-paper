\chapter{基于增量学习的HTTPS新应用识别}\label{chap:transfer}
\section{引言}

\section{迁移学习方法}

\section{增量学习方法}

\subsection{增量学习框架}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.80\textwidth]{IncreAIBMF.pdf}
	\bicaption{IncreAIBMF架构}{IncreAIBMF architecture}
	\label{fig:IncreAIBMF architecture}
\end{figure}



\subsubsection{构建训练集}
This phase prepares the training data to be used in the next training phase. The training set is composed of a portion of the representative samples of the old class (exemplars from the old classes stored in the representative memory) and the samples of the new app class. As our approach uses two loss functions, i.e., classification and distillation, we need two labels for each sample, associated with the two losses. For classification, we use the one-hot vector which indicates the app appearing in the all app classes. For distillation, we use the distillation labels produced by every identification module with old app classes. Therefore, we have as many distillation labels per sample as identification module with old app classes. To better understand, where we are performing the third incremental step of our IncreAIBMF. At this point the two identification module will operate on old apps, and one identification module processes the new apps. When a sample is evaluated, the distillation labels produced by the two identication modules with the old apps are used for distillation, and the the distillation labels produced by the three identification modules are used for classification.


\subsubsection{训练过程}
The training phase takes the augmented training set with its corresponding labels as input for cross-distilled loss computing, and updates all parameters of the entire network architecture. The cross-distilled loss function L(!) is defined as:
\begin{equation}
	\label{eq:cdl}
	L(\omega)=L_{C}(\omega)+\lambda\sum_{f=1}^{F} L_{D_{f}}(\omega)
\end{equation}

where $L_{C}(\omega)$ is the cross-entropy loss applied to samples from the old and new classes. 
%
$L_{D_{f}}$is the distillation loss of the classification layer $f$, and $F$ is the total number of classification layer for the old apps. 
%
$\lambda$ is refered to ratio that distilled loss in the cross-distilled loss. In our training process, we set it 0.3. 
%
The cross-entropy loss $L_{C}(\omega)$ is given by:
%
\begin{equation}
	L_{C}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} p_{i j} \log q_{i j}
\end{equation}
%
where $qi$ is a score obtained by applying a softmax function to the logits of a classification layer for sample $i$, $pi$ is the ground truth for the sample $i$, and $N$ and $C$ denote the number of samples and classes respectively.
%
The distillation loss $L_{D}(\omega)$ is defined as:
%
\begin{equation}
	L_{D}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} { pdist }_{i j} \log {qd i s t}_{i j}
\end{equation}
%
where $pdisti$ and $qdisti$ are modified versions of $pi$ and $qi$, respectively. 

\subsubsection{代表性空间更新}
%
The goal of this phase is to update the sample in the memory, and ensure that the size of the training set does not increase dramatically.
%
That is, in the updated memory, we fixed the memory size to $K$ flows ($K=100,000$ in this paper), and the number of samples corresponding to each category of application is $\frac{100,000}{C}$, here $C$ indicates the category of the application. 
%
Then, we employ the two operations:
%
\emph{(i)} selection of new samples to store: we perform the herding selection strategy \cite{welling2009herding}, which produces a sorted list of samples of one class based on the distance to the mean sample of that class;
%
Given the sorted list of samples, the first $n$ samples of the list are selected. 
%
These samples are most representative of the class according to the mean.
%
% This step is performed after the training process to allocate memory for the samples from the new classes.
%
\emph{(ii)} removal of leftover samples: as the samples are stored in a sorted list, this operation is trivial.
%
The memory unit only needs to remove samples from the end of the sample set of each class. 
%
Note that after this operation, the removed samples are never used again.




\subsection{实验和评估}

\subsubsection{全量数据训练}

\subsubsection{迁移学习效果}

\subsubsection{增量学习表现}



\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.90\textwidth]{Incre-confuse-matrix.pdf}
	\bicaption{IncreAIBM 混淆矩阵}{IncreAIBM confuse matrix}
	\label{fig:IncreAIBM confuse matrix}
\end{figure}


\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.80\textwidth]{Time-cost-incre.pdf}
	\bicaption{IncreAIBMF时间消耗}{Time Cost}
	\label{fig:Incre time cost}
\end{figure}



\section{小节}
