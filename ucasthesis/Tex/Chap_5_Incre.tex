\chapter{基于增量学习的HTTPS新应用识别}\label{chap:transfer}
\section{引言}
目前的加密流量识别方法具有一个主要缺点，那就是新应用程序继续遭受\textit{灾难性遗忘}的困扰，当逐步增加新的应用程序类别进行培训时，整体性能会急剧下降。当前已有的研究工作均是需要全部的流量数据来训练，针对新出现的应用，需要将原来的流量数据和新来的应用的流量数据合并一起重新进行训练，当应用不断出现，训练的样本数量会急剧增加，这将给流量数据的存储以及模型的训练带来巨大的压力。为了解决这个问题，我们建议使用\emph{IncreAIBMF}框架，以使用新的应用程序数据以及仅对应于旧应用程序样本的一小样本集逐步学习深度神经网络。

为了验证我们提出的IncreAIBMF模型的优越性，在本章我们将IncreAIBMF模型分别和迁移学习方法、全量训练进行了比较。

\section{迁移学习方法}
将第四章训练的模型作为基础模型，当新的应用流量数据到来时，采用两种方法：

\begin{itemize}
    \item 将输出层之前的模型参数冻结，仅仅修改输出层的类别数量，使用新的数据连更新全连接层的参数
    \item 修改网络的输出层类别的数量，使用旧模型来来初始化模型输出层之前的参数，即初始化特征提取部分的参数，当新的应用流量数据到来时，重新训练识别模型。
\end{itemize}


\section{增量学习方法}

\subsection{增量学习框架}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.80\textwidth]{IncreAIBMF.pdf}
	\bicaption{IncreAIBMF架构}{IncreAIBMF architecture}
	\label{fig:IncreAIBMF architecture}
\end{figure}



\subsubsection{构建训练集}
此阶段准备训练数据以用于下一个训练阶段。训练集由旧类的代表性样本的一部分（代表存储器中存储的旧类的示例）和新的app类的样本组成。由于我们的方法使用了两个损失函数，即分类和蒸馏，因此每个样本都需要两个标签，与这两个损失相关联。为了进行分类，我们使用一键向量来指示应用程序出现在所有应用程序类中。对于蒸馏，我们使用每个识别模块使用旧的应用程序类生成的蒸馏标签。因此，每个样品的蒸馏标签与具有旧应用程序类的识别模块一样多。为了更好地理解，我们在哪里执行IncreAIBMF的第三步增量。此时，两个标识模块将在旧应用程序上运行，一个标识模块将处理新应用程序。对样品进行评估时，将使用旧应用程序的两个识别模块生成的蒸馏标签用于蒸馏，将使用三个识别模块生成的蒸馏标签用于分类。


\subsubsection{训练过程}
训练阶段将带有相应标签的增强训练集作为交叉蒸馏损失计算的输入，并更新整个网络体系结构的所有参数。 交叉蒸馏损失函数L（！）定义为：
\begin{equation}
	\label{eq:cdl}
	L(\omega)=L_{C}(\omega)+\lambda\sum_{f=1}^{F} L_{D_{f}}(\omega)
\end{equation}

where $L_{C}(\omega)$ is the cross-entropy loss applied to samples from the old and new classes. 
%
$L_{D_{f}}$is the distillation loss of the classification layer $f$, and $F$ is the total number of classification layer for the old apps. 
%
$\lambda$ is refered to ratio that distilled loss in the cross-distilled loss. In our training process, we set it 0.3. 
%
The cross-entropy loss $L_{C}(\omega)$ is given by:
%
\begin{equation}
	L_{C}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} p_{i j} \log q_{i j}
\end{equation}
%
where $qi$ is a score obtained by applying a softmax function to the logits of a classification layer for sample $i$, $pi$ is the ground truth for the sample $i$, and $N$ and $C$ denote the number of samples and classes respectively.
%
精馏损失 $L_{D}(\omega)$ 定义为:
%
\begin{equation}
	L_{D}(\omega)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} { pdist }_{i j} \log {qd i s t}_{i j}
\end{equation}
%
where $pdisti$ and $qdisti$ are modified versions of $pi$ and $qi$, respectively. 

\subsubsection{代表性空间更新}
%
The goal of this phase is to update the sample in the memory, and ensure that the size of the training set does not increase dramatically.
%
That is, in the updated memory, we fixed the memory size to $K$ flows ($K=100,000$ in this paper), and the number of samples corresponding to each category of application is $\frac{100,000}{C}$, here $C$ indicates the category of the application. 
%
Then, we employ the two operations:
%
\emph{(i)} selection of new samples to store: we perform the herding selection strategy \cite{welling2009herding}, which produces a sorted list of samples of one class based on the distance to the mean sample of that class;
%
Given the sorted list of samples, the first $n$ samples of the list are selected. 
%
These samples are most representative of the class according to the mean.
%
% This step is performed after the training process to allocate memory for the samples from the new classes.
%
\emph{(ii)} removal of leftover samples: as the samples are stored in a sorted list, this operation is trivial.
%
The memory unit only needs to remove samples from the end of the sample set of each class. 
%
Note that after this operation, the removed samples are never used again.




\subsection{实验和评估}
我们使用第四章所训练的AIBMF模型筛选出原始的代表性空间，并将该空间的大小设置为10条样本，筛选的方法为通过该模型计算出全连接层的向量，计算出所有样本计算的全连接层向量的中心点，保留前$100000/K$的距离中心点最近的样本，在这里100000表示的是我们的代表性空间的大小，$K$代表的是类别数目，如一开始$K=20$，则在构建代表性空间的时候，每一个应用被保留的样本的数量将会$\leq 5000$，即每一个应用距离中心点的前5000个样本会被保留，不够5000的将会被全部保留。


\subsubsection{全量数据训练}
为了验证我们模型在所有50个应用20多万条流的样本上识别效果，我们针对50个应用一次性进行训练，其中18条流用于训练，2万多条流用于验证。


\subsubsection{迁移学习效果}
迁移学习是一种处理新数据的有效方法，它普遍使用的场景为：原始模型是通过大量的数据训练得到的，处理的新数据是更为小范围的数据，即原始模型本身训练了更多的数据，具有更强的特征建模能力。在我们的研究过程中，应用不断出现，原始模型的数量要远远少于新增数据的数量，另外实验结果表明，微调的方法并不能够很好处理本研究课题面对的新增应用数量巨大带来的问题。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.90\textwidth]{AIBMF-Fine-tuning.pdf}
	\bicaption{AIBMF 微调}{AIBMF Fine tuning}
	\label{fig:AIBMF-Fine-tuning}
\end{figure}

\subsubsection{增量学习表现}



\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.90\textwidth]{Incre-confuse-matrix.pdf}
	\bicaption{IncreAIBM 混淆矩阵}{IncreAIBM confuse matrix}
	\label{fig:IncreAIBM confuse matrix}
\end{figure}


\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Full-comparation.eps}
      \caption{}
      \label{fig:oaspl_a}
    \end{subfigure}%
    ~% add desired spacing
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Incre-comparation.eps}
      \caption{}
      \label{fig:oaspl_b}
    \end{subfigure}
    ~% add desired spacing

    \bicaption{总声压级。(a) 这是子图说明信息，(b) 这是子图说明信息。}{OASPL.(a) This is the explanation of subfig, (b) This is the explanation of subfig.}
    \label{fig:oaspl}
\end{figure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.90\textwidth]{Time-cost-incre.eps}
	\bicaption{AIBMF和IncreAIBM时间消耗对比 }{Time cost comparation between AIBMF and IncreAIBMF}
	\label{fig:Time-cost-incre}
\end{figure}


\section{小节}
这一部分提出了IncreAIBMF模型，有效处理新增应用的流量数据的识别，使得我们的识别模型能够有效克服灾难遗忘问题，在新增应用的识别任务上，能够有效节约时间和空间，更加搞笑完成识别任务。