\chapter{数据集和数据预处理}\label{chap:collect}
在课题研究过程中，需要大量的数据支持。需要在大量的数据上训练移动应用流量识别模型以及验证模型的效果。然而，当前所公布的开源数据较少，数据量有限，分类的粒度较为粗糙。

\section{公开流量数据集}

\citep{draper2016characterization}发布了ISCX数据集\footnote{https://www.unb.ca/cic/datasets/vpn.html}，其中包括7种常规加密流量和7种协议封装流量。

\citep{bujlow2015independent}提出数据集\footnote{https://cba.upc.edu/monitoring/traffic-classification}

of [45,47,49,51,53,60


DARPA99 traces\footnote{https://www.ll.mit.edu/r-d/datasets}: The DARPA99 traces are packets from
a simulated network at the MIT Lincoln Laboratory in
1999. All network data is available in these packets and so
identification was done reliably using the SSH handshake.

MAWI Working Group Traffic\footnote{http://mawi.wide.ad.jp/mawi/}


NLANR AMP Data\footnote{https://labs.ripe.net/datarepository/data-sets/nlanr-amp-data}This data is a set of active measurements (ping/traceroute) collected by the NLANR research group. The data consists of measurements in a mesh of up to 130 vantage points and the measurements ran between 1998 and 2006. This data is useful for longitudinal study of the Internet. The data available from the RIPE Data Repository is a remastered version of the original data that was available from the NLANR website.


AMP & MAWI traces: These two traces are public captures with no payload information available. 




本课题的目的在于通过对大量的应用的HTTPS流量数据进行分析，得到一种可靠准确的分析方法，以上提到的公开数据集，规模有限且对于数据的标注不符合我们的要求。

\section{流量采集}
在本文章中介绍一种针对Android应用流量的自动化/半自动化的流量采集方法。数据采集的流程如图所示，该系统主要由APP爬虫、应用调度、App运行环境、事件注入和守护进程五个模块构成。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.80\textwidth]{Data-collect.pdf}
	\bicaption{数据采集}{Data collect}
	\label{fig:data_collect}
\end{figure}

\section{预处理}
我们以流为单位进行研究和标记，离线的数据流结构如图，我们在采集流量的时候，保存的同一个pcap文件是由多条流构成的。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.80\textwidth]{flow_offline}
	\bicaption{离线流数据}{}
	\label{fig:flow_offline}
\end{figure}

\subsection{基于splitcap抽取流}
SplitCap\footnote{https://www.netresec.com/?page=SplitCap}是一个免费工具，旨在根据诸如IP地址，5元组或MAC地址等标准将捕获文件（PCAP文件）拆分为较小的文件。 可用于拆分/分组的标准是：
\begin{itemize}
    \item BSSID：根据WLAN BSSID分组的数据包
    \item 流：每个5元组的单向流量（传输协议，IP地址和端口号）分组在一起。
    \item 主机：将流量按IP地址（源和目标）分组到一个文件。 大多数数据包将以两个文件结尾。
    \item 主机对：基于IP对通信进行分组的流量。
    \item MAC地址：将流量按每个MAC地址分组到一个文件。 大多数数据包将以两个文件结尾。
    \item 会话：每个会话的数据包（双向流）被分组在一起。
    \item 时间：根据时间拆分。
    \item 数据包计数：根据数据包计数拆分。
\end{itemize}
\begin{lstlisting}[language=sh]
foreach($f in gci 1_Pcap *.pcap)
{
    SplitCap -p 100000 -b 100000 -r $f.FullName -o 2_Session\AllLayers\$($f.BaseName)-ALL
    SplitCap -p 100000 -b 100000 -r $f.FullName -s flow -o 2_Session\AllLayers\$($f.BaseName)-ALL
    gci 2_Session\AllLayers\$($f.BaseName)-ALL | ?{$_.Length -eq 0} | del
}
\end{lstlisting}

\subsection{基于Scapy抽取流}
Scapy是一个Python程序，使用户能够发送，嗅探，剖析和伪造网络数据包。 此功能允许构建可以探测，扫描或攻击网络的工具。Scapy是功能强大的交互式数据包处理程序。 它能够伪造或解码各种协议的数据包，在线发送它们，捕获它们，匹配请求和答复等等。 Scapy可以轻松处理大多数经典任务，例如扫描，跟踪路由，探测，单元测试，攻击或网络发现。 它可以替代hping，arpspoof，arp-sk，arping，p0f甚至Nmap，tcpdump和tshark的某些部分。
\begin{lstlisting}[language=Python]
#!/usr/bin/env python
#encoding=utf-8
"""
@author: TianMao
@contact: tianmao1994@yahoo.com
@file: split-pcap.py
@time: 19-11-27 下午6:38
@desc:
    功能：按照TCP流切分pcap文件
    参考：https://github.com/mao-tool/packet-analysis
    环境：Scapy
    使用：python split-pcap.py test.pcap
    输出：test.pcap_220.194.64.35-443_192.168.137.22-56458_split.pcap
    问题：当前输出为splitcap文件的一般？疑似这里处理的是双向流？
"""

import sys
import re
import glob

# This is needed to suppress a really irrating warning message when scapy
# is imported
import logging
logging.getLogger("scapy.runtime").setLevel(logging.ERROR)



try:
    from scapy.all import*
except ImportError:
    print "scapy is not installed. See comments for installation suggestions"
    exit ()

# argument processing, require just the file name. If a second argument
# is provided make sure its an integer
if len (sys.argv) < 2 or len (sys.argv) > 3:
   print "Usage is: split-pcap.py file-name [packet-count]"
   print "Try\n     grep -A 20 Usage: " + sys.argv[0] +  \
                                            " | head -20\nfor details"
   exit ()

if len (sys.argv) == 3:
   inputFileString = sys.argv [1]
   try:
      inputTotalPackets = int (sys.argv [2])
   except ValueError:
      print "The second argument must be an integer <" + \
                       sys.argv [2] + "> does appear to be an integer"
      exit ()
else:
   inputFileString = sys.argv [1]
   inputTotalPackets = 0


# 保存文件夹
out_dir = "../../../../data/1/raw_2/"

# try opening the file.
try:
   pcapIn = PcapReader (inputFileString)
except IOError:
   print "It doesn't look like " + inputFileString + " exists"
   exit()
except NameError:
   print "It doesn't look like " + inputFileString + \
                                      " is a file that can be processed."
   print "Note that this script cannot process pcapng files. Review the "
   print "usage details for ideas on how to convert from pcapng to pcap"
   exit ()

# Extract out just the the file name. Note that I assume the the ".*/" match
# is greedy and will match until the last "/" character in the string. If
# the match fails there are no "/" characters so the whole string must be the
# name.
x = re.search ("^.*/(.*$)", inputFileString)
try:
   prefix = x.group(1) + "_"
except:
   prefix = inputFileString + "_"

# Look for prefix*_split.pcap files. If you find them print a
# warning and exit.

t = len (glob (prefix + "*_split.pcap"))
if t > 0:
   print "There are already " + str (t) + " files with the name " + \
       prefix + "*_split.pcap."
   print "Delete or rename them or change to a different directory to"
   print "avoid adding duplicate packets into the " + prefix + \
                                               "*_split.pcap trace files."
   exit ()

# 判断是否存在当前文件的文件夹
if not os.path.exists(out_dir + inputFileString):
    os.makedirs(out_dir + inputFileString)


pcapOutName = ""
oldPcapOutName = ""
packetCount = 0
donePercentage = 0;
oldDonePercentage = -1

# Loop for each packet in the file

for aPkt in pcapIn:

# count the packets read
   packetCount = packetCount + 1

# If the packet contains a TCP header extract out the IP addresses and
# port numbers
   if TCP in aPkt:
      ipSrc = aPkt[IP].src
      tcpSport = aPkt[TCP].sport
      ipDst = aPkt[IP].dst
      tcpDport = aPkt[TCP].dport

# put things in some sort of cannonical order. It doesn't really matter
# what the order is as long as packets going in either direction get the
# same order.
      if ipSrc > ipDst:
         pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap"
      elif ipSrc < ipDst:
         pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap"
      elif tcpSport > tcpDport:
         pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap"
      else:
         pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap"

# If the current packet should be written to a different file from the last
# packet, close the current output file and open the new file for append
# save the name of the newly opened file so we can compare it for the next
# packet.
      if pcapOutName != oldPcapOutName:
         if oldPcapOutName != "":
            pcapOut.close()

         if type(aPkt) == scapy.layers.l2.Ether:
            lkType = 1
         elif type (aPkt) == scapy.layers.l2.CookedLinux:
            lkType = 113
         else:
            print "Unknown link type: "
            type (aPkt)
            print "    -- exiting"
            exit

         # 修改文件路劲
         pcapOutName = out_dir+inputFileString+"/"+pcapOutName
         pcapOut = PcapWriter (pcapOutName, linktype=lkType, append=True)
         oldPcapOutName = pcapOutName

# write the packet
      pcapOut.write (aPkt)

# Write the progress information, either percentages if we had a packet-count
# argument or just the packet count.

      if inputTotalPackets > 0:
         donePercentage = packetCount * 100 / inputTotalPackets
         if donePercentage > oldDonePercentage:
            print "Percenage done: ", donePercentage
            oldDonePercentage = donePercentage
      else:
         print packetCount
\end{lstlisting}

\subsection{数据集概览}
我们基于HTTPS协议创建流量数据集。 数据集包含20个流行应用程序的100,000多个HTTPS流。 我们在多个真实的Android设备和Android模拟器中都执行一个应用程序。 该应用由Android工具monkeyrunner\footnote{https://developer. android. com/studio/test/monkeyrunner/index.html}自动驱动。 我们会同时在不同设备上执行同一应用，并通过限制android设备中其他应用的权限来确保没有应用在后台运行。 我们一次捕获一个应用程序的流量，以确保它是事实。 为了避免受到网络环境的影响，我们在一个月的时间里，一直在手动和自动方式下通过Wireshark \footnote{https://www.wireshark.org/}和TcpDump\footnote{http://www.tcpdump.org}来捕获流量。


共采集了旅行交通、社交、影音视听、时尚购物新闻资讯、居家生活、聊天、图书阅读、金融理财、实用工具、游戏共计11种类型，50个应用的流量，使用sliptcap切分后得到20万多万条流量样本。


\begin{longtable}{c|c|c|c}
    \bicaption{Android应用}{Android apps}\\
	\hline
	\textbf{应用名} & \textbf{开发商} & \textbf{应用类别} & \textbf{应用数量} \\
	\hline
	\endfirsthead
	\multicolumn{4}{c}%
        {\bfseries\small \tablename\ \thetable\ {续表。}}\\
	\hline
	\textbf{应用名} & \textbf{开发商} & \textbf{应用类别} & \textbf{应用数量} \\
	\hline
	\endhead
	\hline \multicolumn{4}{r}{\textit{续表见下页}}\\
	\endfoot
	\hline
	\endlastfoot
	百度地图 & Baidu.com & 旅行交通 & 6777\\
	\hline
	百度贴吧 & Baidu.com & 社交 & 3234\\
	\hline
	网易云音乐 & Netease.com & 影音视听 & 9888\\
	\hline
	爱奇艺 &  iQIYI & 影音视听 & 2634\\
	\hline
	京东 & JD & 购物 & 7956\\
	\hline
	今日头条 & ByteDance & 新闻资讯 & 5321\\
	\hline
	美团 & Meituan.com& 居家生活 & 12469\\
	\hline
	QQ & Tencent & 聊天 & 1246\\
	\hline
	QQ音乐 & Tencent & 影音视听 & 1155\\
	\hline
	QQ阅读 & Tencent & 图书阅读 & 1563\\
	\hline
	淘宝 & Taobao & 购物 & 3431\\
	\hline
	微博 & Sina & 社交 & 3097\\
	\hline
	携程 & CTRIP & 居家生活 & 2141\\
	\hline
	知乎 & Zhihu.com & 社交 & 2011\\
	\hline
	抖音 & Douyin.com & 社交 & 7441 \\
	\hline
	饿了么 & Ele.me & 居家生活 & 16053\\
	\hline
	国泰君安 & gtja.com & 金融理财 & 7734\\
	\hline
	QQ邮箱 & Tencent & 实用工具 & 4879\\
	\hline
	腾讯新闻 & Tencent & 新闻资讯 & 5679\\
	\hline
	支付宝 & Alipay.com & 金融理财 & 2301\\
	\hline
	阿里健康 & alihealth.cn & 居家生活 & 22904\\
	\hline
	安居客 & anjuke.com & 居家生活 & 2340\\
	\hline
	百词斩 & baicizhan.com & 实用工具 & 674\\
	\hline
	百合婚恋 &  baihe.com & 居家生活 & 2452\\
	\hline
	贝壳找房 & bj.ke.com & 居家生活 & 7520\\
	\hline
	当当阅读 & dangdang.com & 图书阅读 & 2588\\
	\hline
	钉钉 & dingtalk.com & 居家生活 & 3468\\
	\hline
	丁香 & dxy.cn & 居家生活 & 4654\\
	\hline
	豆瓣 & douban.com & 社交 & 3998\\
	\hline
	火山小视频 & huoshan.com & 影音视听 & 2924\\
	\hline
	Keep & www.gotokeep.com & 居家生活 & 9646\\
	\hline
	秒拍 & miaopai.com & 社交 & 340\\
	\hline
	中国南方航空 & China Southern Airlines & 旅行交通 & 7656\\
	\hline
	拼多多 & pinduoduo.com & 购物 & 2660\\
	\hline
	蜻蜓FM & 影音视听 & 社交 & 2312 \\
	\hline
	去哪儿 & qunar.com & 居家生活 & 3294\\
	\hline
	Soul & soulapp.cn & 聊天 & 4406\\
	\hline
	天涯 & tianya.cn & 聊天 & 1058\\
	\hline
	天眼查 & tianyancha.com & 实用工具 & 6146\\
	\hline
	同花顺 & 10jqka.com.cn & 金融理财 & 5928\\
	\hline
	王者荣耀 & Tencent & 游戏 & 2524\\
	\hline
	闲鱼 & 2.taobao.com & 购物 & 3226\\
	\hline
	小米运动 & huami.com & 居家生活 & 2364\\
	\hline
	新浪财经 & 金融理财.sina.com.cn & 金融理财 & 3608\\
	\hline
	央视新闻 & news.cctv.com & 社交 & 1534 \\
	\hline
	有道云笔记 & note.youdao.com & 实用工具 & 2396\\
	\hline
	掌上生活 & cmbchina.com & 金融理财 & 4838\\
	\hline
	直播吧 & zhibo8.cc & 实用工具 & 4368\\
	\hline
	中国国际航空 & 中国国际航空 & 旅行交通 & 3343\\
	\hline
	作业帮 & Zybang.com & 实用工具 & 4692\\
	\hline
	\hline
	\textbf{总计} & - & - & \textbf{236871}\\
	\hline
\end{longtable}

\section{小结}


% 湖南大学的《安卓手机应用流量分析恶意行为检测技术研究》


% \begin{figure}[!htbp]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{data_collect_algo1}
% 	\bicaption{深度优先搜索Android程序的执行路径}{}
% 	\label{fig:data_collect_algo_1}
% \end{figure}

% \subsection{移动终端应用与行为识别与技术研究——肖新光}

% \begin{figure}[!htbp]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{data_collect_algo_2}
% 	\bicaption{西电硕士论文}{}
% 	\label{fig:data_collect_algo_2}
% \end{figure}


% \subsection{自动化的标注}
% 一种思路是一次性仅仅标注一个或者非常有限的程序，这样可以借助一些特定的规则，比如端口号，SNI进行标注。或者借助认为的判断，但是对于应用程序很多的情况而言，这种方案不现实。所以在产生数据的时候更加倾向于一次性开启很少的程序，虚拟化网卡，在这个网卡上抓取应用产生的流量，这样可以避免其他程序，如操作系统带来的干扰。或者使用如iptable的技术来控制ip，当然这样做的前提还是局限在单次抓取的流量很少的情况。

% 对于混合流量，尝试使用L7-filter和GT工具来标注混合数据集。L7-filter的实现是基于特征的关键字匹配，采用正则表达式的方式来对关键字进行描述和匹配，这种方法对于协议的识别有着更高的效率和准确率。GT是一款开源的网络流分析工具，主要包括四个部分：(1)gt客户端进程：运行在每个被监控的网络节点上，从主机内核获取每一个应用的名称，并且记录每一个应用所存在的数据流信息(2)数据包捕获用来在网络的路由器处捕获所有被监控的主机的数据包(3)数据库服务器用来存储gt客户端上应用名和对应的数据流信息(4)数据集标记：将捕获的应用数据根据应用名和对应的数据流信息进行标记，并且根据具体的应用将数据流进行分区。
% \begin{figure}[!htbp]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{gt_structure}
% 	\bicaption{开源的gt工具}{}
% 	\label{fig:gt_structure}
% \end{figure}
% \url{http://netweb.ing.unibs.it/~ntw/实用工具/gt/}

% 开源的数据集：\href{http://tstat.tlc.polito.it/}{TCP STatistic and Analysis Tool}，\href{http://mawi.wide.ad.jp/mawi/samplepoint-F/2018/201809021400.html}{pcap格式抓包数据}

