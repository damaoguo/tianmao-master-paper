\chapter{数据集和数据预处理}\label{chap:collect}
在课题研究过程中，需要大量的数据支持。需要在大量的数据上训练移动应用流量识别模型以及验证模型的效果。然而，当前所公布的开源数据较少，数据量有限，分类的粒度较为粗糙。本章详细地描述了移动应用HTTPS流量数据地采集和处理方法，构建了一个50个不同类型的移动应用的流量数据集，规模为20万条流样本。

\section{公开流量数据集}
本章首先本文调研了当前已经公开的流量数据集，总结这些公开数据集的来源、规模、协议、用途以及获取方式。现有的公开并被多项研究工作使用的数据集如下：
\begin{itemize}
    \item \citep{draper2016characterization}发布了VPN-nonVPN dataset (ISCXVPN2016)数据集\footnote{https://www.unb.ca/cic/datasets/vpn.html}，其中包括7种常规加密流量和7种协议封装流量。该项工作捕获了一个常规会话和一个通过VPN的会话，因此，共有14种流量类别：VOIP，VPN-VOIP，P2P，VPN-P2P等。数据内容包括：
    请见表~\ref{tab:sample}。
    \begin{table}[!htbp]
        \bicaption{这是一个样表。}{This is a sample table.}
        \label{tab:sample}
        \centering
        \footnotesize% fontsize
        \setlength{\tabcolsep}{4pt}% column separation
        \renewcommand{\arraystretch}{1.2}%row space 
        \begin{tabular}{lc}
            \hline
            类型 & 内容 \\
            \hline
            Traffic & Content \\
            Web Browsing & Firefox and Chrome \\
            Email & SMPTS, POP3S and IMAPS \\
            Chat & ICQ, AIM, Skype, Facebook and Hangouts \\
            Streaming & Vimeo and Youtube \\
            File Transfer & Skype, FTPS and SFTP using Filezilla and an external service \\
            VoIP & Facebook, Skype and Hangouts voice calls (1h duration) \\
            P2P & uTorrent and Transmission (Bittorrent)\\
            \hline
        \end{tabular}
    \end{table}
    
    \item Intrusion detection evaluation dataset (ISCXIDS2012) \footnote{https://www.unb.ca/cic/datasets/ids.html}：该数据集包括标记的网络跟踪，包括pcap格式的完整数据包有效负载，以及相关的配置文件供研究人员公开使用。如表：\ref{tab:ISCXIDS2012}所示，由7天的网络活动（正常和恶意）组成：
     \begin{table}[!htbp]
        \bicaption{这是一个样表。}{This is a sample table.}
        \label{tab:ISCXIDS2012}
        \centering
        \footnotesize% fontsize
        \setlength{\tabcolsep}{4pt}% column separation
        \renewcommand{\arraystretch}{1.2}%row space 
        \begin{tabular}{lcc}
        \hline
        \textbf{日期} &  \textbf{描述} & \textbf{数据规模(GB)}\\
        \hline
        11/6/2010 & Normal Activity. No malicious activity & 16.1\\
        12/6/2010 & Normal Activity. No malicious activity & 4.22\\
        13/6/2010 & Infiltrating the network from inside + Normal Activity & 3.95\\
        14/6/2010 & HTTP Denial of Service + Normal Activity & 6.85\\
        15/6/2010 & Distributed Denial of Service using an IRC Botnet & 23.4\\
        16/6/2010 & Normal Activity. No malicious activity & 17.6\\
        17/6/2010 & Brute Force SSH + Normal Activity & 12.3\\
        \hline
        \end{tabular}
    \end{table}
    该数据集合主要针对恶意流量的分析，包含了HTTP，SSH等协议诸如DDOS，Brute Force等恶意行为，因此该数据集在本文研究的任务重合度有限。
    
    \item \citep{bujlow2015independent}提出数据集Independent Comparison of Popular DPI Tools for Traffic Classification" dataset\footnote{https://cba.upc.edu/monitoring/traffic-classification}，这个数据集包含767690个流，这些流占53.31GB的纯数据包数据。存在759720个流的应用程序名称（占所有流的98.96％），占数据量的51.93GB（97.41％）。数据集由一个pcap跟踪和一个INFO文件组成。INFO文件中的每一行对应于pcap跟踪中的流，并描述如下：
    \begin{lstlisting}
     flow_id + "#" + start_time + "#" + end_time + "#" + local_ip + "#" + remote_ip + "#" + local_port + "#" + remote_port + "#" + transport_protocol + "#" + operating_system + "#" + process_name + "#" + HTTP Url + "#" + HTTP Referer + "#" + HTTP Content-type +"#"
    \end{lstlisting}
    本数据集数据规模较大，但是数据是已经预处理，对于新的识别问题存在局限。
    
    \item DARPA99 traces\footnote{https://www.ll.mit.edu/r-d/datasets}:该ARPA99跟踪是来自MIT Lincoln实验室1999年的模拟网络的数据包。这些数据包中提供了所有网络数据，其标签是不同的攻击行为，因此该数据集不符合本文的研究场景。
    
    \item MAWILab\footnote{http://mawi.wide.ad.jp/mawi/}：\citep{mawilab}发布了MAWILab数据集，可帮助研究人员评估其交通异常检测方法。它由一组用于定位MAWI存档中交通异常的标签（样本点B和F）组成。使用基于图形的高级方法获得标签，该方法比较并组合了不同且独立的异常检测器。 数据集每天更新，以包括来自即将到来的应用程序和异常的新流量。该数据集是用于评估异常流量检测方法的，不符合本文的研究场景。
    
    \item NLANR AMP Data\footnote{https://labs.ripe.net/datarepository/data-sets/nlanr-amp-data}：此数据是NLANR研究小组收集的一组活动测量（ping /traceroute）。 数据由多达130个有利点的网格中的测量组成，并且这些测量在1998年至2006年之间进行。该数据可用于Internet的纵向研究。RIPE数据存储库中的可用数据是NLANR网站上可用的原始数据的重新制作版本。该数据集包括的是\emph{ping/traceroute}和相关的流量，不符合本文的研究场景所需。
    
    \item NIMS\footnote{https://projects.cs.dal.ca/projectx/Download.html}：在研究工作中\citep{alshammari2011can,alshammari2008investigating,alshammari2007flow}提出了NIMS数据集，该数据的标签为：\emph{TELNET, FTP, HTTP, DNS, lime, localForwarding, remoteForwarding, scp, sftp, x11, shell}，是基于协议层的分类，且单条样本已经被预处理为22个字段的数值，主要为均值、方差、最大值、最小值等统计特征。该数据集涉及了多种加密协议的样本，在识别粒度上要比本文的识别任务粗糙，不符合本文场景。
    
    \item WITS: Waikato Internet Traffic Storage\footnote{https://wand.net.nz/wits/}：该数据集目前有33个不同的集合构成，数据规模较为庞大，且数据来源较为丰富，但是该数据集采集的是混合流量，样本缺少精细化的标记，因此难于用于训练精细化的流量识别方法，无法满足本文中的精细化识别场景的需求。
    
    
\end{itemize}


本课题的目的在于通过对大量的应用的HTTPS流量数据进行分析，得到一种可靠准确的分析方法，经过分析，以上提到的公开数据集在通信协议、数据规模、数据标签、数据格式或者使用场景等不满足本文针对HTTPS流量进行精细化识别的要求。为此需要构建新的数据集来支撑本文的研究工作。

\section{流量采集}
在本文章中介绍一种针对Android应用流量的自动化/半自动化的流量采集方法。数据采集的流程如图所示，该系统主要由APP爬虫、应用调度、App运行环境、事件注入和守护进程五个模块构成。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.80\textwidth]{Data-collect.pdf}
	\bicaption{数据采集}{Data collect}
	\label{fig:data_collect}
\end{figure}

\section{预处理}
我们以流为单位进行研究和标记，离线的数据流结构如图，我们在采集流量的时候，保存的同一个pcap文件是由多条流构成的。



\subsection{基于splitcap抽取流}
SplitCap\footnote{https://www.netresec.com/?page=SplitCap}是一个免费工具，旨在根据诸如IP地址，5元组或MAC地址等标准将捕获文件（PCAP文件）拆分为较小的文件。 可用于拆分/分组的标准是：
\begin{itemize}
    \item BSSID：根据WLAN BSSID分组的数据包
    \item 流：每个5元组的单向流量（传输协议，IP地址和端口号）分组在一起。
    \item 主机：将流量按IP地址（源和目标）分组到一个文件。 大多数数据包将以两个文件结尾。
    \item 主机对：基于IP对通信进行分组的流量。
    \item MAC地址：将流量按每个MAC地址分组到一个文件。 大多数数据包将以两个文件结尾。
    \item 会话：每个会话的数据包（双向流）被分组在一起。
    \item 时间：根据时间拆分。
    \item 数据包计数：根据数据包计数拆分。
\end{itemize}
\begin{lstlisting}[language=sh]
foreach($f in gci 1_Pcap *.pcap)
{
    SplitCap -p 100000 -b 100000 -r $f.FullName -o 2_Session\AllLayers\$($f.BaseName)-ALL
    SplitCap -p 100000 -b 100000 -r $f.FullName -s flow -o 2_Session\AllLayers\$($f.BaseName)-ALL
    gci 2_Session\AllLayers\$($f.BaseName)-ALL | ?{$_.Length -eq 0} | del
}
\end{lstlisting}

\subsection{基于Scapy抽取流}
Scapy是一个Python程序，使用户能够发送，嗅探，剖析和伪造网络数据包。 此功能允许构建可以探测，扫描或攻击网络的工具。Scapy是功能强大的交互式数据包处理程序。 它能够伪造或解码各种协议的数据包，在线发送它们，捕获它们，匹配请求和答复等等。 Scapy可以轻松处理大多数经典任务，例如扫描，跟踪路由，探测，单元测试，攻击或网络发现。 它可以替代hping，arpspoof，arp-sk，arping，p0f甚至Nmap，tcpdump和tshark的某些部分。
\begin{lstlisting}[language=Python]
#!/usr/bin/env python
#encoding=utf-8
"""
@desc:
    功能：按照TCP流切分pcap文件
    参考：https://github.com/mao-tool/packet-analysis
    环境：Scapy
    使用：python split-pcap.py test.pcap
    输出：test.pcap_220.194.64.35-443_192.168.137.22-56458_split.pcap
    问题：当前输出为splitcap文件的一般？疑似这里处理的是双向流？
"""

import sys
import re
import glob

# This is needed to suppress a really irrating warning message when scapy
# is imported
import logging
logging.getLogger("scapy.runtime").setLevel(logging.ERROR)



try:
    from scapy.all import*
except ImportError:
    print "scapy is not installed. See comments for installation suggestions"
    exit ()

# argument processing, require just the file name. If a second argument
# is provided make sure its an integer
if len (sys.argv) < 2 or len (sys.argv) > 3:
   print "Usage is: split-pcap.py file-name [packet-count]"
   print "Try\n     grep -A 20 Usage: " + sys.argv[0] +  \
                                            " | head -20\nfor details"
   exit ()

if len (sys.argv) == 3:
   inputFileString = sys.argv [1]
   try:
      inputTotalPackets = int (sys.argv [2])
   except ValueError:
      print "The second argument must be an integer <" + \
                       sys.argv [2] + "> does appear to be an integer"
      exit ()
else:
   inputFileString = sys.argv [1]
   inputTotalPackets = 0


# 保存文件夹
out_dir = "../../../../data/1/raw_2/"

# try opening the file.
try:
   pcapIn = PcapReader (inputFileString)
except IOError:
   print "It doesn't look like " + inputFileString + " exists"
   exit()
except NameError:
   print "It doesn't look like " + inputFileString + \
                                      " is a file that can be processed."
   print "Note that this script cannot process pcapng files. Review the "
   print "usage details for ideas on how to convert from pcapng to pcap"
   exit ()

# Extract out just the the file name. Note that I assume the the ".*/" match
# is greedy and will match until the last "/" character in the string. If
# the match fails there are no "/" characters so the whole string must be the
# name.
x = re.search ("^.*/(.*$)", inputFileString)
try:
   prefix = x.group(1) + "_"
except:
   prefix = inputFileString + "_"

# Look for prefix*_split.pcap files. If you find them print a
# warning and exit.

t = len (glob (prefix + "*_split.pcap"))
if t > 0:
   print "There are already " + str (t) + " files with the name " + \
       prefix + "*_split.pcap."
   print "Delete or rename them or change to a different directory to"
   print "avoid adding duplicate packets into the " + prefix + \
                                               "*_split.pcap trace files."
   exit ()

# 判断是否存在当前文件的文件夹
if not os.path.exists(out_dir + inputFileString):
    os.makedirs(out_dir + inputFileString)


pcapOutName = ""
oldPcapOutName = ""
packetCount = 0
donePercentage = 0;
oldDonePercentage = -1

# Loop for each packet in the file

for aPkt in pcapIn:

# count the packets read
   packetCount = packetCount + 1

# If the packet contains a TCP header extract out the IP addresses and
# port numbers
   if TCP in aPkt:
      ipSrc = aPkt[IP].src
      tcpSport = aPkt[TCP].sport
      ipDst = aPkt[IP].dst
      tcpDport = aPkt[TCP].dport

# put things in some sort of cannonical order. It doesn't really matter
# what the order is as long as packets going in either direction get the
# same order.
      if ipSrc > ipDst:
         pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap"
      elif ipSrc < ipDst:
         pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap"
      elif tcpSport > tcpDport:
         pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap"
      else:
         pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap"

# If the current packet should be written to a different file from the last
# packet, close the current output file and open the new file for append
# save the name of the newly opened file so we can compare it for the next
# packet.
      if pcapOutName != oldPcapOutName:
         if oldPcapOutName != "":
            pcapOut.close()

         if type(aPkt) == scapy.layers.l2.Ether:
            lkType = 1
         elif type (aPkt) == scapy.layers.l2.CookedLinux:
            lkType = 113
         else:
            print "Unknown link type: "
            type (aPkt)
            print "    -- exiting"
            exit

         # 修改文件路劲
         pcapOutName = out_dir+inputFileString+"/"+pcapOutName
         pcapOut = PcapWriter (pcapOutName, linktype=lkType, append=True)
         oldPcapOutName = pcapOutName

# write the packet
      pcapOut.write (aPkt)

# Write the progress information, either percentages if we had a packet-count
# argument or just the packet count.

      if inputTotalPackets > 0:
         donePercentage = packetCount * 100 / inputTotalPackets
         if donePercentage > oldDonePercentage:
            print "Percenage done: ", donePercentage
            oldDonePercentage = donePercentage
      else:
         print packetCount
\end{lstlisting}

原始的流量数据经过处理以后，一个原始的pacp文件将会按照流拆分为新的pcap小文件，文件存储的目录结构如图：\ref{fig:file-tree}所示。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.80\textwidth]{File-tree.pdf}
	\bicaption{流数据文件存储目录}{Traffic Flow File Tree}
	\label{fig:file-tree}
\end{figure}



\subsection{数据集概览}
我们基于HTTPS协议创建流量数据集。 数据集包含20个流行应用程序的100,000多个HTTPS流。 我们在多个真实的Android设备和Android模拟器中都执行一个应用程序。 该应用由Android工具monkeyrunner\footnote{https://developer. android. com/studio/test/monkeyrunner/index.html}自动驱动。 我们会同时在不同设备上执行同一应用，并通过限制android设备中其他应用的权限来确保没有应用在后台运行。 我们一次捕获一个应用程序的流量，以确保它是事实。 为了避免受到网络环境的影响，我们在一个月的时间里，一直在手动和自动方式下通过Wireshark \footnote{https://www.wireshark.org/}和TcpDump\footnote{http://www.tcpdump.org}来捕获流量。


共采集了旅行交通、社交、影音视听、时尚购物新闻资讯、居家生活、聊天、图书阅读、金融理财、实用工具、游戏共计11种类型，50个应用的流量，使用sliptcap切分后得到20万多万条流量样本。


\begin{longtable}{c|c|c|c}
    \bicaption{Android应用}{Android apps}\\
	\hline
	\textbf{应用名} & \textbf{开发商} & \textbf{应用类别} & \textbf{应用数量} \\
	\hline
	\endfirsthead
	\multicolumn{4}{c}%
        {\bfseries\small \tablename\ \thetable\ {续表。}}\\
	\hline
	\textbf{应用名} & \textbf{开发商} & \textbf{应用类别} & \textbf{应用数量} \\
	\hline
	\endhead
	\hline \multicolumn{4}{r}{\textit{续表见下页}}\\
	\endfoot
	\hline
	\endlastfoot
	百度地图 & Baidu.com & 旅行交通 & 6777\\
	\hline
	百度贴吧 & Baidu.com & 社交 & 3234\\
	\hline
	网易云音乐 & Netease.com & 影音视听 & 9888\\
	\hline
	爱奇艺 &  iQIYI & 影音视听 & 2634\\
	\hline
	京东 & JD & 购物 & 7956\\
	\hline
	今日头条 & ByteDance & 新闻资讯 & 5321\\
	\hline
	美团 & Meituan.com& 居家生活 & 12469\\
	\hline
	QQ & Tencent & 聊天 & 1246\\
	\hline
	QQ音乐 & Tencent & 影音视听 & 1155\\
	\hline
	QQ阅读 & Tencent & 图书阅读 & 1563\\
	\hline
	淘宝 & Taobao & 购物 & 3431\\
	\hline
	微博 & Sina & 社交 & 3097\\
	\hline
	携程 & CTRIP & 居家生活 & 2141\\
	\hline
	知乎 & Zhihu.com & 社交 & 2011\\
	\hline
	抖音 & Douyin.com & 社交 & 7441 \\
	\hline
	饿了么 & Ele.me & 居家生活 & 16053\\
	\hline
	国泰君安 & gtja.com & 金融理财 & 7734\\
	\hline
	QQ邮箱 & Tencent & 实用工具 & 4879\\
	\hline
	腾讯新闻 & Tencent & 新闻资讯 & 5679\\
	\hline
	支付宝 & Alipay.com & 金融理财 & 2301\\
	\hline
	阿里健康 & alihealth.cn & 居家生活 & 22904\\
	\hline
	安居客 & anjuke.com & 居家生活 & 2340\\
	\hline
	百词斩 & baicizhan.com & 实用工具 & 674\\
	\hline
	百合婚恋 &  baihe.com & 居家生活 & 2452\\
	\hline
	贝壳找房 & bj.ke.com & 居家生活 & 7520\\
	\hline
	当当阅读 & dangdang.com & 图书阅读 & 2588\\
	\hline
	钉钉 & dingtalk.com & 居家生活 & 3468\\
	\hline
	丁香 & dxy.cn & 居家生活 & 4654\\
	\hline
	豆瓣 & douban.com & 社交 & 3998\\
	\hline
	火山小视频 & huoshan.com & 影音视听 & 2924\\
	\hline
	Keep & www.gotokeep.com & 居家生活 & 9646\\
	\hline
	秒拍 & miaopai.com & 社交 & 340\\
	\hline
	中国南方航空 & China Southern Airlines & 旅行交通 & 7656\\
	\hline
	拼多多 & pinduoduo.com & 购物 & 2660\\
	\hline
	蜻蜓FM & 影音视听 & 社交 & 2312 \\
	\hline
	去哪儿 & qunar.com & 居家生活 & 3294\\
	\hline
	Soul & soulapp.cn & 聊天 & 4406\\
	\hline
	天涯 & tianya.cn & 聊天 & 1058\\
	\hline
	天眼查 & tianyancha.com & 实用工具 & 6146\\
	\hline
	同花顺 & 10jqka.com.cn & 金融理财 & 5928\\
	\hline
	王者荣耀 & Tencent & 游戏 & 2524\\
	\hline
	闲鱼 & 2.taobao.com & 购物 & 3226\\
	\hline
	小米运动 & huami.com & 居家生活 & 2364\\
	\hline
	新浪财经 & 金融理财.sina.com.cn & 金融理财 & 3608\\
	\hline
	央视新闻 & news.cctv.com & 社交 & 1534 \\
	\hline
	有道云笔记 & note.youdao.com & 实用工具 & 2396\\
	\hline
	掌上生活 & cmbchina.com & 金融理财 & 4838\\
	\hline
	直播吧 & zhibo8.cc & 实用工具 & 4368\\
	\hline
	中国国际航空 & 中国国际航空 & 旅行交通 & 3343\\
	\hline
	作业帮 & Zybang.com & 实用工具 & 4692\\
	\hline
	\hline
	\textbf{总计} & - & - & \textbf{236871}\\
	\hline
\end{longtable}

\section{小结}


% 湖南大学的《安卓手机应用流量分析恶意行为检测技术研究》


% \begin{figure}[!htbp]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{data_collect_algo1}
% 	\bicaption{深度优先搜索Android程序的执行路径}{}
% 	\label{fig:data_collect_algo_1}
% \end{figure}

% \subsection{移动终端应用与行为识别与技术研究——肖新光}

% \begin{figure}[!htbp]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{data_collect_algo_2}
% 	\bicaption{西电硕士论文}{}
% 	\label{fig:data_collect_algo_2}
% \end{figure}


% \subsection{自动化的标注}
% 一种思路是一次性仅仅标注一个或者非常有限的程序，这样可以借助一些特定的规则，比如端口号，SNI进行标注。或者借助认为的判断，但是对于应用程序很多的情况而言，这种方案不现实。所以在产生数据的时候更加倾向于一次性开启很少的程序，虚拟化网卡，在这个网卡上抓取应用产生的流量，这样可以避免其他程序，如操作系统带来的干扰。或者使用如iptable的技术来控制ip，当然这样做的前提还是局限在单次抓取的流量很少的情况。

% 对于混合流量，尝试使用L7-filter和GT工具来标注混合数据集。L7-filter的实现是基于特征的关键字匹配，采用正则表达式的方式来对关键字进行描述和匹配，这种方法对于协议的识别有着更高的效率和准确率。GT是一款开源的网络流分析工具，主要包括四个部分：(1)gt客户端进程：运行在每个被监控的网络节点上，从主机内核获取每一个应用的名称，并且记录每一个应用所存在的数据流信息(2)数据包捕获用来在网络的路由器处捕获所有被监控的主机的数据包(3)数据库服务器用来存储gt客户端上应用名和对应的数据流信息(4)数据集标记：将捕获的应用数据根据应用名和对应的数据流信息进行标记，并且根据具体的应用将数据流进行分区。
% \begin{figure}[!htbp]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{gt_structure}
% 	\bicaption{开源的gt工具}{}
% 	\label{fig:gt_structure}
% \end{figure}
% \url{http://netweb.ing.unibs.it/~ntw/实用工具/gt/}

% 开源的数据集：\href{http://tstat.tlc.polito.it/}{TCP STatistic and Analysis Tool}，\href{http://mawi.wide.ad.jp/mawi/samplepoint-F/2018/201809021400.html}{pcap格式抓包数据}

